{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "from utils import configuration\n",
    "from utils.utils import *\n",
    "from load_data import load_data\n",
    "from trainer import trainer\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    ")\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "cfg = 'config/uda_re.json'\n",
    "model_cfg = 'config/bert_base.json'\n",
    "cfg = configuration.params.from_json(cfg)\n",
    "model_cfg = configuration.model.from_json(model_cfg)\n",
    "set_seeds(cfg.seed)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "data = load_data(cfg)\n",
    "#sup_train_iter = data.sup_data_iter()\n",
    "# unzsup_train_iter = data.unsup_data_iter()\n",
    "sup_test_iter = data.test_data_iter()\n",
    "# print([len(loader)  for loader in (sup_train_iter, unzsup_train_iter, sup_test_iter)])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "# load model\n",
    "config = AutoConfig.from_pretrained(model_cfg.model_name_or_path,num_labels=model_cfg.num_labels)\n",
    "model = AutoModelForSequenceClassification.from_config(config=config)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device('cuda')\n",
    "model = model.to(device)\n",
    "\n",
    "## for unsup\n",
    "# criterion = nn.KLDivLoss(reduction='batchmean', log_target=True)\n",
    "# LSM = nn.LogSoftmax(dim=1)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=2e-3)\n",
    "#model = AutoModelForSequenceClassification.from_config(config=config)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "import logging\n",
    "\n",
    "\n",
    "epochs = 1\n",
    "model.train()\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "for n in range(epochs):\n",
    "    for step, batch in enumerate(sup_test_iter):\n",
    "        # end 조건\n",
    "        if step > cfg.ratio * len(sup_test_iter):\n",
    "            break\n",
    "\n",
    "        #  sup data를 device에 담는다.\n",
    "        sup_input_ids, sup_input_mask, sup_input_type_ids, label_ids = (t.to(device) for t in batch)\n",
    "\n",
    "        # inputs에 따른 outputs을 낸다\n",
    "        sup_outputs = model(sup_input_ids, sup_input_mask, sup_input_type_ids)\n",
    "\n",
    "        sup_loss = criterion(sup_outputs.logits, label_ids)\n",
    "\n",
    "        # backpropagation\n",
    "        sup_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # logging\n",
    "        if step % 10 == 0:\n",
    "            logging.info(f'Currnent train step: {step}/{int(len(sup_test_iter) * cfg.ratio)}')\n",
    "            logging.info(f'Currnent sup loss : {sup_loss}')\n",
    "\n",
    "logging.info('Train end')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:root:Currnent train step: 0/78\n",
      "INFO:root:Currnent sup loss : 0.6672449707984924\n",
      "INFO:root:Currnent train step: 10/78\n",
      "INFO:root:Currnent sup loss : 0.7843418121337891\n",
      "INFO:root:Currnent train step: 20/78\n",
      "INFO:root:Currnent sup loss : 0.6474961638450623\n",
      "INFO:root:Currnent train step: 30/78\n",
      "INFO:root:Currnent sup loss : 0.6686989068984985\n",
      "INFO:root:Currnent train step: 40/78\n",
      "INFO:root:Currnent sup loss : 0.7316548824310303\n",
      "INFO:root:Currnent train step: 50/78\n",
      "INFO:root:Currnent sup loss : 0.7148677110671997\n",
      "INFO:root:Currnent train step: 60/78\n",
      "INFO:root:Currnent sup loss : 0.7111068367958069\n",
      "INFO:root:Currnent train step: 70/78\n",
      "INFO:root:Currnent sup loss : 0.6714945435523987\n",
      "INFO:root:Train end\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#model_load(model, cfg, path='model/')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "UDA_trainer = trainer(model, cfg)\n",
    "data_iter = {'sup_test' : sup_test_iter}\n",
    "accuracy = UDA_trainer.test(data_iter)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:root:Test start\n",
      "INFO:root:pred : tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0') labels : tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,\n",
      "        1, 1, 0, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "INFO:root:Currnent test step: 0/78\n",
      "INFO:root:Currnent accuracy : 10/32:  0.31\n",
      "INFO:root:Total accuracy : 10/32:  0.31\n",
      "INFO:root:pred : tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0') labels : tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,\n",
      "        1, 0, 1, 1, 0, 1, 0, 0], device='cuda:0')\n",
      "INFO:root:Currnent test step: 10/78\n",
      "INFO:root:Currnent accuracy : 16/32:  0.50\n",
      "INFO:root:Total accuracy : 168/352:  0.48\n",
      "INFO:root:pred : tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0') labels : tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,\n",
      "        1, 0, 1, 1, 1, 0, 1, 0], device='cuda:0')\n",
      "INFO:root:Currnent test step: 20/78\n",
      "INFO:root:Currnent accuracy : 14/32:  0.44\n",
      "INFO:root:Total accuracy : 339/672:  0.50\n",
      "INFO:root:pred : tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0') labels : tensor([1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 1, 0, 1, 0, 1, 1], device='cuda:0')\n",
      "INFO:root:Currnent test step: 30/78\n",
      "INFO:root:Currnent accuracy : 17/32:  0.53\n",
      "INFO:root:Total accuracy : 494/992:  0.50\n",
      "INFO:root:pred : tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0], device='cuda:0') labels : tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "INFO:root:Currnent test step: 40/78\n",
      "INFO:root:Currnent accuracy : 14/32:  0.44\n",
      "INFO:root:Total accuracy : 653/1312:  0.50\n",
      "INFO:root:pred : tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0') labels : tensor([1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 1, 0], device='cuda:0')\n",
      "INFO:root:Currnent test step: 50/78\n",
      "INFO:root:Currnent accuracy : 20/32:  0.62\n",
      "INFO:root:Total accuracy : 823/1632:  0.50\n",
      "INFO:root:pred : tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0') labels : tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
      "        1, 0, 0, 1, 1, 0, 1, 1], device='cuda:0')\n",
      "INFO:root:Currnent test step: 60/78\n",
      "INFO:root:Currnent accuracy : 16/32:  0.50\n",
      "INFO:root:Total accuracy : 979/1952:  0.50\n",
      "INFO:root:pred : tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0') labels : tensor([1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
      "        1, 0, 1, 0, 1, 1, 0, 1], device='cuda:0')\n",
      "INFO:root:Currnent test step: 70/78\n",
      "INFO:root:Currnent accuracy : 14/32:  0.44\n",
      "INFO:root:Total accuracy : 1140/2272:  0.50\n",
      "INFO:root:Test end\n",
      "INFO:root:Total accuracy :   0.51\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "a = torch.tensor([1, 2, 3], dtype=float, requires_grad=True)\n",
    "model.no_grad()\n",
    "for i, v in enumerate(model.parameters()):\n",
    "    print(v)\n",
    "    if i == 1:\n",
    "        break\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# sup train\n",
    "\n",
    "# sup_data_iter = itertools.cycle(sup_test_iter)\n",
    "model.train()\n",
    "epochs = 100\n",
    "losses = []\n",
    "for n in range(epochs):\n",
    "    for steps, batch in enumerate(sup_train_iter):\n",
    "        inputs = (t.to(device) for t in batch[:3])\n",
    "        labels = batch[-1].to(device)\n",
    "        outputs = model(*inputs)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        print(f'steps : {n * len(sup_train_iter) + steps + 1} / {epochs * len(sup_train_iter)} loss : {loss}')\n",
    "\n",
    "        pred = torch.argmax(outputs.logits, dim = 1)\n",
    "        print(f'pred {pred} label {labels}')\n",
    "        print(f'result : {torch.sum(pred == labels, dim = 0)}/ {len(labels)}')\n",
    "        loss.backward()\n",
    "        losses.append(loss)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "PATH = \"model/train.pt\"\n",
    "torch.save(model.state_dict(), PATH)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "PATH = \"model/train.pt\"\n",
    "model.load_state_dict(torch.load(PATH))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(dpi=150)\n",
    "plt.plot(losses)\n",
    "plt.title('supervised')\n",
    "plt.xlabel('steps')\n",
    "plt.ylabel('loss')\n",
    "plt.show()\n",
    "plt.savefig('fig/superviese_losses.png', dpi=3000)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# del labels\n",
    "# del inputs\n",
    "# del outputs\n",
    "# del loss\n",
    "# del pred\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "'''\n",
    "unsup_data_iter = itertools.cycle(sup_train_iter)\n",
    "for steps, batch in enumerate(unsup_data_iter):\n",
    "    ori_outputs = model(*batch[:3])\n",
    "    aug_outputs = model(*batch[3:])\n",
    "    loss = criterion(LSM(aug_outputs.logits), LSM(ori_outputs.logits))\n",
    "    print(f'steps : {steps} loss : {loss}')\n",
    "    ori_pred = torch.argmax(ori_outputs.logits, dim = 1)\n",
    "    aug_pred = torch.argmax(aug_outputs.logits, dim = 1)\n",
    "    print(f'ori_pred {ori_pred} aug_pred {aug_pred}')\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "'''"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "\n",
    "device1 = torch.device('cuda')\n",
    "print(device1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved\n",
    "print(t,r,a,f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def sharpening_prediction(i : torch.Tensor, temperature:float, log : bool = True):\n",
    "    i = i / temperature\n",
    "    if log:\n",
    "        f = torch.nn.LogSoftmax(dim=1)\n",
    "    else:\n",
    "        f = torch.nn.Softmax(dim=1)\n",
    "    return f(i)\n",
    "\n",
    "def confidence_based_masking(x: torch.Tensor, beta:float):\n",
    "    f = torch.nn.LogSoftmax(dim=1)\n",
    "    y = f(x)\n",
    "    maxPs, _ = torch.max(y, dim = 1)\n",
    "    \n",
    "    pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "\n",
    "a = torch.rand((10, 2)) * 10\n",
    "b = sharpening_prediction(a, 0.5)\n",
    "print(a, b, end = '')\n",
    "max_b, _ = torch.max(b, dim=1)\n",
    "uses_batch_id = max_b > 0.5\n",
    "print(uses_batch_id)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "a = torch.tensor([1, 2, 3, 4, 5])\n",
    "print(a.data)\n",
    "print(a)\n",
    "print(math.log(0.5))\n",
    "print(math.log(0.8))"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('uda': conda)"
  },
  "interpreter": {
   "hash": "961342292b0d5152411d449f6b406d5074b05bb69c5a71f49c708121cac2a35f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}