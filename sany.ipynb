{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from utils import configuration\r\n",
    "from utils.utils import set_seeds\r\n",
    "from load_data import load_data\r\n",
    "\r\n",
    "import torch.optim as optim\r\n",
    "import torch.nn as nn\r\n",
    "import torch\r\n",
    "\r\n",
    "from transformers import (\r\n",
    "    AutoConfig,\r\n",
    "    AutoModelForSequenceClassification,\r\n",
    ")\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "cfg = 'config/uda.json'\r\n",
    "model_cfg = 'config/bert_base.json'\r\n",
    "cfg = configuration.params.from_json(cfg)\r\n",
    "model_cfg = configuration.model.from_json(model_cfg)\r\n",
    "set_seeds(cfg.seed)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "data = load_data(cfg)\r\n",
    "sup_train_iter = data.sup_data_iter()\r\n",
    "# unzsup_train_iter = data.unsup_data_iter()\r\n",
    "# sup_test_iter = data.eval_data_iter()\r\n",
    "# print([len(loader)  for loader in (sup_train_iter, unzsup_train_iter, sup_test_iter)])\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# load model\r\n",
    "config = AutoConfig.from_pretrained(model_cfg.model_name_or_path,num_labels=model_cfg.num_labels)\r\n",
    "model = AutoModelForSequenceClassification.from_config(config=config)\r\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\r\n",
    "criterion = nn.CrossEntropyLoss()\r\n",
    "device = torch.device('cuda')\r\n",
    "model = model.to(device)\r\n",
    "\r\n",
    "## for unsup\r\n",
    "# criterion = nn.KLDivLoss(reduction='batchmean', log_target=True)\r\n",
    "# LSM = nn.LogSoftmax(dim=1)\r\n",
    "#optimizer = optim.Adam(model.parameters(), lr=2e-3)\r\n",
    "#model = AutoModelForSequenceClassification.from_config(config=config)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "a = torch.tensor([1, 2, 3], dtype=float, requires_grad=True)\r\n",
    "model.no_grad()\r\n",
    "for i, v in enumerate(model.parameters()):\r\n",
    "    print(v)\r\n",
    "    if i == 1:\r\n",
    "        break\r\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'BertForSequenceClassification' object has no attribute 'no_grad'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-11cae416f9e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\uda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1129\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[1;32m-> 1131\u001b[1;33m             type(self).__name__, name))\n\u001b[0m\u001b[0;32m   1132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Module'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BertForSequenceClassification' object has no attribute 'no_grad'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# sup train\r\n",
    "\r\n",
    "# sup_data_iter = itertools.cycle(sup_test_iter)\r\n",
    "model.train()\r\n",
    "epochs = 100\r\n",
    "losses = []\r\n",
    "for n in range(epochs):\r\n",
    "    for steps, batch in enumerate(sup_train_iter):\r\n",
    "        inputs = (t.to(device) for t in batch[:3])\r\n",
    "        labels = batch[-1].to(device)\r\n",
    "        outputs = model(*inputs)\r\n",
    "        loss = criterion(outputs.logits, labels)\r\n",
    "        print(f'steps : {n * len(sup_train_iter) + steps + 1} / {epochs * len(sup_train_iter)} loss : {loss}')\r\n",
    "\r\n",
    "        pred = torch.argmax(outputs.logits, dim = 1)\r\n",
    "        print(f'pred {pred} label {labels}')\r\n",
    "        print(f'result : {torch.sum(pred == labels, dim = 0)}/ {len(labels)}')\r\n",
    "        loss.backward()\r\n",
    "        losses.append(loss)\r\n",
    "        optimizer.step()\r\n",
    "        optimizer.zero_grad()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "PATH = \"model/train.pt\"\r\n",
    "torch.save(model.state_dict(), PATH)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "PATH = \"model/train.pt\"\r\n",
    "model.load_state_dict(torch.load(PATH))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "plt.figure(dpi=150)\r\n",
    "plt.plot(losses)\r\n",
    "plt.title('supervised')\r\n",
    "plt.xlabel('steps')\r\n",
    "plt.ylabel('loss')\r\n",
    "plt.show()\r\n",
    "plt.savefig('fig/superviese_losses.png', dpi=3000)\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# del labels\r\n",
    "# del inputs\r\n",
    "# del outputs\r\n",
    "# del loss\r\n",
    "# del pred\r\n",
    "# torch.cuda.empty_cache()\r\n",
    "\r\n",
    "'''\r\n",
    "unsup_data_iter = itertools.cycle(sup_train_iter)\r\n",
    "for steps, batch in enumerate(unsup_data_iter):\r\n",
    "    ori_outputs = model(*batch[:3])\r\n",
    "    aug_outputs = model(*batch[3:])\r\n",
    "    loss = criterion(LSM(aug_outputs.logits), LSM(ori_outputs.logits))\r\n",
    "    print(f'steps : {steps} loss : {loss}')\r\n",
    "    ori_pred = torch.argmax(ori_outputs.logits, dim = 1)\r\n",
    "    aug_pred = torch.argmax(aug_outputs.logits, dim = 1)\r\n",
    "    print(f'ori_pred {ori_pred} aug_pred {aug_pred}')\r\n",
    "    loss.backward()\r\n",
    "    optimizer.step()\r\n",
    "'''"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "import torch\r\n",
    "\r\n",
    "device1 = torch.device('cuda')\r\n",
    "print(device1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "t = torch.cuda.get_device_properties(0).total_memory\r\n",
    "r = torch.cuda.memory_reserved(0)\r\n",
    "a = torch.cuda.memory_allocated(0)\r\n",
    "f = r-a  # free inside reserved\r\n",
    "print(t,r,a,f)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4294967296 0 0 0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def sharpening_prediction(i : torch.Tensor, temperature:float, log : bool = True):\r\n",
    "    i = i / temperature\r\n",
    "    if log:\r\n",
    "        f = torch.nn.LogSoftmax(dim=1)\r\n",
    "    else:\r\n",
    "        f = torch.nn.Softmax(dim=1)\r\n",
    "    return f(i)\r\n",
    "\r\n",
    "def confidence_based_masking(x: torch.Tensor, beta:float):\r\n",
    "    f = torch.nn.LogSoftmax(dim=1)\r\n",
    "    y = f(x)\r\n",
    "    maxPs, _ = torch.max(y, dim = 1)\r\n",
    "    \r\n",
    "    pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "import torch\r\n",
    "\r\n",
    "a = torch.rand((10, 2)) * 10\r\n",
    "b = sharpening_prediction(a, 0.5)\r\n",
    "print(a, b, end = '')\r\n",
    "max_b, _ = torch.max(b, dim=1)\r\n",
    "uses_batch_id = max_b > 0.5\r\n",
    "print(uses_batch_id)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[8.6023, 9.0657],\n",
      "        [1.4684, 6.4086],\n",
      "        [1.8653, 5.9751],\n",
      "        [8.6026, 5.3899],\n",
      "        [6.7140, 5.2371],\n",
      "        [6.6462, 8.0812],\n",
      "        [1.8522, 3.2141],\n",
      "        [5.9392, 3.2032],\n",
      "        [3.4509, 4.8256],\n",
      "        [8.4353, 3.8566]]) tensor([[-1.2603e+00, -3.3348e-01],\n",
      "        [-9.8805e+00, -5.1139e-05],\n",
      "        [-8.2198e+00, -2.6926e-04],\n",
      "        [-1.6186e-03, -6.4270e+00],\n",
      "        [-5.0826e-02, -3.0047e+00],\n",
      "        [-2.9251e+00, -5.5151e-02],\n",
      "        [-2.7873e+00, -6.3565e-02],\n",
      "        [-4.1939e-03, -5.4762e+00],\n",
      "        [-2.8113e+00, -6.2010e-02],\n",
      "        [-1.0549e-04, -9.1574e+00]])tensor([False, False, False, False, False, False, False, False, False, False])\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('uda': conda)"
  },
  "interpreter": {
   "hash": "c83a7510ad18b29dfa5117131711e2c63bed04f56a8aa80dd1a92ae7b310bfb3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}